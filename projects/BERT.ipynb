{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“modelv4.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29555e6b1f2a4ada830d448e9f618c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bdbb0ea404de4566bbcfd7c32947cd3c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_506111f85efa46e189e4435e78660061",
              "IPY_MODEL_61483652f680497bb0b7d2ff1f568daa"
            ]
          }
        },
        "bdbb0ea404de4566bbcfd7c32947cd3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "506111f85efa46e189e4435e78660061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c2786c09199947278776e5efeae4a29e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_379e1f9cc8214009b0810ddc58ee5abe"
          }
        },
        "61483652f680497bb0b7d2ff1f568daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bccd2bc0760547abb305026582a196b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.61MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e89a029090f04d92bd6289a93e13cdf8"
          }
        },
        "c2786c09199947278776e5efeae4a29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "379e1f9cc8214009b0810ddc58ee5abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bccd2bc0760547abb305026582a196b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e89a029090f04d92bd6289a93e13cdf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwSWA7XDD40s",
        "colab_type": "text"
      },
      "source": [
        "Sub Task A results: https://arxiv.org/pdf/1903.08983.pdf\n",
        "\n",
        "Data from: https://github.com/sandro272/SemEval2019-OffensEval/tree/51dde8c38b512d5fb536fd74b2afd3dc7ed73831/train_data\n",
        "\n",
        "https://github.com/sandro272/SemEval2019-OffensEval/tree/51dde8c38b512d5fb536fd74b2afd3dc7ed73831/test_data\n",
        "\n",
        "pre-processing: https://github.com/sandro272/SemEval2019-OffensEval/blob/51dde8c38b512d5fb536fd74b2afd3dc7ed73831/code/demo.py#L9\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "senFKVtyUALS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "e849a964-87b8-4fce-c0e9-85faca1a34bc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 19.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=e9449ef52e496d56152b85d021ee12520628d850ce8862d1f03baddbc3c16c35\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMppP12JUSVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch   \n",
        "from torchtext import data, datasets \n",
        "from torchtext.data import TabularDataset \n",
        "import pandas as pd\n",
        "from torchtext.vocab import Vectors\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "from torchtext.vocab import Vectors\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup,get_constant_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "from transformers import BertTokenizer,BertModel\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig,AdamWeightDecay\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfdnNp0EIB07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c9930a41-21ba-4943-84b7-0e5ce0611714"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVP72Eb1IGxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "63371a6e-14d4-4281-9851-894a6d8247ce"
      },
      "source": [
        "!ls \"/content/drive/My Drive/576-project\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Check-In #1.docx'\t\t        modelv4.ipynb\n",
            "'Check in #2.docx'\t\t        “prepocess_data.ipynb”的副本\n",
            "'Final Write-Up COSC576 Project.docx'   prepocess_emoji.py\n",
            "'model4 result with diff hyper.docx'    prepocess.ipynb\n",
            " model_bestweights_GRU.pth\t       'Presentation Project COSC576.pptx'\n",
            " modelv1.ipynb\t\t\t       'Project Proposal COSC576.docx'\n",
            " modelv2.ipynb\t\t\t        testdata.csv\n",
            " modelv3.ipynb\t\t\t        traindata.csv\n",
            " “modelv3-updated.ipynb”\t        train_subtask_a.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeyLr9y_IVpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(2020)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riS74LzPDOpN",
        "colab_type": "text"
      },
      "source": [
        "optimizer &learning rate scheduler\n",
        "\n",
        "**hyperparameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DdfO4ffLnDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "learning_rate = 2e-5\n",
        "eps = 3e-7\n",
        "batch_size = 64\n",
        "bert_type = 'bert-base-uncased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMOEw19KmiA",
        "colab_type": "text"
      },
      "source": [
        "read traindata and testdata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuh5ZCuLIW2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindata = pd.read_csv('/content/drive/My Drive/576-project/traindata.csv')\n",
        "testdata = pd.read_csv('/content/drive/My Drive/576-project/testdata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls9FXb_sIwXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "72cc69f7-fcfd-4c86-e3c6-bc21cf6c83ec"
      },
      "source": [
        "traindata.head()\n",
        "testdata.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WhoIsQ  WheresTheServer  DumpNike  DECLASFISA...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ConstitutionDay is revered by Conservatives  ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOXNews  NRA  MAGA  POTUS  TRUMP  2ndAmendmen...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Watching  Boomer getting the news that she is...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NoPasaran  Unity demo to oppose the far right...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label\n",
              "0   WhoIsQ  WheresTheServer  DumpNike  DECLASFISA...      1\n",
              "1   ConstitutionDay is revered by Conservatives  ...      0\n",
              "2   FOXNews  NRA  MAGA  POTUS  TRUMP  2ndAmendmen...      0\n",
              "3   Watching  Boomer getting the news that she is...      0\n",
              "4   NoPasaran  Unity demo to oppose the far right...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNAT7UkPIlgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "train_sentences = traindata.tweet.values\n",
        "train_labels = traindata.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqZ-mgmJU5-",
        "colab_type": "text"
      },
      "source": [
        "tokenization & input formatting\n",
        "1. bert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzRj2wB9JTcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "29555e6b1f2a4ada830d448e9f618c4f",
            "bdbb0ea404de4566bbcfd7c32947cd3c",
            "506111f85efa46e189e4435e78660061",
            "61483652f680497bb0b7d2ff1f568daa",
            "c2786c09199947278776e5efeae4a29e",
            "379e1f9cc8214009b0810ddc58ee5abe",
            "bccd2bc0760547abb305026582a196b3",
            "e89a029090f04d92bd6289a93e13cdf8"
          ]
        },
        "outputId": "a4778513-39b2-4902-b594-f595a456eb28"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(bert_type, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29555e6b1f2a4ada830d448e9f618c4f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrpqVW-sM4o3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b9acb73-07e9-4954-d639-80c749e2ce73"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "for sent in train_sentences:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrhnqWP_QqgJ",
        "colab_type": "text"
      },
      "source": [
        "since the longest sentences here is 115, set the maximum length to 128. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMEX7DBWQps9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in train_sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmTF8ioxK8M6",
        "colab_type": "text"
      },
      "source": [
        "create train and validation set (the proportion is 8:2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFA3UfqCTiSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "21de4797-0e32-4db3-cd60-894e78d7b93a"
      },
      "source": [
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10,592 training samples\n",
            "2,648 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFZj465aUzsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset,sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "validation_dataloader = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),batch_size = batch_size )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF-5V3VDVvMv",
        "colab_type": "text"
      },
      "source": [
        " Use BertForSequenceClassification\n",
        "\n",
        " refer: https://huggingface.co/transformers/v2.2.0/model_doc/bert.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwnaPp3ZgAsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        bo = self.bert_drop(o2)\n",
        "        output = self.out(bo)\n",
        "        return(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riig5VBUVs6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BERTBaseUncased()\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjOVvOhMYkpT",
        "colab_type": "text"
      },
      "source": [
        "train the classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XArAzUfJxQQi",
        "colab_type": "text"
      },
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPO_WLddDEGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),lr = learning_rate,eps=eps)\n",
        "#optimizer = AdamWeightDecay(lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds35z8vAV3yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler =  get_cosine_schedule_with_warmup(optimizer,num_warmup_steps = 1.8,num_training_steps = total_steps,num_cycles=0.6) \n",
        "#scheduler =  get_constant_schedule_with_warmup(optimizer,num_warmup_steps = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUhOo5cVMMN7",
        "colab_type": "text"
      },
      "source": [
        "training loop\n",
        "\n",
        " This training code is based on the `run_glue.py` script here:\n",
        "https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGBW-CsMLMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_val = 576\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4yxZ65Mk7fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_stats = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUAxMBFJmd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpEC6pkg7vs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1score(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    TP = np.sum((labels_flat == 1) & (pred_flat == 1))\n",
        "    FP = np.sum((labels_flat == 0) & (pred_flat == 1))\n",
        "    FN = np.sum((labels_flat == 1) & (pred_flat == 0))  \n",
        "    precision = TP / (TP + FP)\n",
        "    #precision_0\n",
        "    recall = TP / (TP + FN)\n",
        "    #recall_0 =\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return np.sum(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCKu3nIYpvXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1_score_macro(preds, labels):\n",
        "\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    f1 = f1_score(labels_flat, pred_flat,average='macro')\n",
        "    return np.sum(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkVgP4SfpPYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    total_train_f1 = 0\n",
        "    total_train_f1_macro = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.   '.format(step, len(train_dataloader)))\n",
        "            print(\"  batch loss: {0:.4f}\".format(total_train_loss/step))\n",
        "            if(total_train_loss/step < 0.45):\n",
        "               break\n",
        "            \n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        (loss, logits) = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy() \n",
        "       \n",
        "\n",
        "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
        "        total_train_f1 += f1score(logits, label_ids)\n",
        "        total_train_f1_macro += f1_score_macro(logits, label_ids)\n",
        "        \n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        " \n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "    \n",
        "    avg_train_f1 = total_train_f1 / len(train_dataloader)\n",
        "    print(\"  Average training F1: {0:.4f}\".format(avg_train_f1))   \n",
        "\n",
        "    avg_train_f1_macro = total_train_f1_macro / len(train_dataloader)\n",
        "    print(\"  Average training macro-F1: {0:.4f}\".format(avg_train_f1_macro))   \n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_eval_f1 = 0\n",
        "    total_eval_f1_macro = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        with torch.no_grad():        \n",
        "            (loss, logits) = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)     \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        total_eval_f1 += f1score(logits, label_ids)\n",
        "\n",
        "        total_eval_f1_macro += f1_score_macro(logits, label_ids)\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader) \n",
        "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        "\n",
        "    avg_val_f1 = total_eval_f1 / len(validation_dataloader) \n",
        "    print(\"  Validation F1: {0:.4f}\".format(avg_val_f1))\n",
        "\n",
        "    avg_val_f1_macro = total_eval_f1_macro / len(validation_dataloader) \n",
        "    print(\"  Validation macro-F1: {0:.4f}\".format(avg_val_f1_macro))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Train. Accur.': avg_train_accuracy,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Train F1.': avg_train_f1,\n",
        "            'Valid F1.': avg_val_f1,\n",
        "            'Train macro-F1':avg_train_f1_macro,\n",
        "            'Valid macro-F1':avg_val_f1_macro,\n",
        "        }\n",
        "    )\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRdP8W7Y_Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs8GlzYRV8wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "\n",
        "ax1.plot(df_stats['epoch'],df_stats['Valid. Loss'],'-o', label='validation loss')\n",
        "ax1.plot(df_stats['epoch'],df_stats['Training Loss'],'-o', label='training loss')\n",
        "\n",
        "\n",
        "\n",
        "ax1.set_xlabel('Epoch (s)')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Loss')\n",
        "ax1.set_xticks([1,2,3,4])\n",
        "#ax1.set_xlim([0, 5])\n",
        "ax1.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9uYFMpiT92e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 3, 1)\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "ax3 = fig.add_subplot(2, 3, 3)\n",
        "\n",
        "ax1.plot(df_stats['Valid. Loss'], label='validation loss')\n",
        "ax1.plot(df_stats['Training Loss'], label='training loss')\n",
        "\n",
        "ax2.plot(df_stats['Train. Accur.'], label='training accuracy')\n",
        "ax2.plot(df_stats['Valid. Accur.'], label='validation accuracy')\n",
        "\n",
        "ax3.plot(df_stats['Train macro-F1'], label='training macro-F1-score.')\n",
        "ax3.plot(df_stats['Valid macro-F1'], label='validation macro-F1-score.')\n",
        "\n",
        "\n",
        "ax1.set_xlabel('Epoch (s)')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Loss')\n",
        "ax1.set_xticks([1,2,3,4])\n",
        "ax1.legend()\n",
        "\n",
        "\n",
        "ax2.set_xlabel('Epoch (s)')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.set_xticks([1,2,3,4])\n",
        "ax2.legend()\n",
        "\n",
        "ax3.set_xlabel('Epoch (s)')\n",
        "ax3.set_ylabel('macro-F1')\n",
        "ax3.set_title('macro-F1')\n",
        "ax3.set_xticks([1,2,3,4])\n",
        "ax3.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wqq4cVetn0",
        "colab_type": "text"
      },
      "source": [
        "test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME18XkATetJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = testdata.tweet.values\n",
        "labels = testdata.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D95JfhAhT-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "        \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "  \n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kFwgN42Znm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
        "\n",
        "def TP(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    TP = np.sum((labels_flat == 1) & (pred_flat == 1))\n",
        "    return(TP)\n",
        "def FP(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    FP = np.sum((labels_flat == 0) & (pred_flat == 1))\n",
        "    return(FP)\n",
        "def FN(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    FN = np.sum((labels_flat == 1) & (pred_flat == 0))\n",
        "    return(FN)\n",
        "def TN(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    TN = np.sum((labels_flat == 0) & (pred_flat == 0))\n",
        "    return(TN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unW9h5COae_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "# Predict \n",
        "total_test_accuracy = 0\n",
        "total_test_f1 = 0\n",
        "total_test_f1_macro = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "tn = 0\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy().to\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  total_test_accuracy += flat_accuracy(logits, label_ids)\n",
        "  avg_test_accuracy = total_test_accuracy / len(prediction_dataloader)\n",
        "  \n",
        "  total_test_f1 += f1score(logits, label_ids)\n",
        "  avg_test_f1 = total_test_f1 / len(prediction_dataloader)\n",
        "  test_f1 = f1score(logits, label_ids)\n",
        "  #print(\" test f1: {0:.4f}\".format(test_f1))\n",
        "  \n",
        "\n",
        "  total_test_f1_macro += f1_score_macro(logits, label_ids)\n",
        "  avg_test_f1_macro = total_test_f1_macro / len(prediction_dataloader)\n",
        "  test_f1_macro = f1_score_macro(logits, label_ids)\n",
        "  #print(\" test f1_macro: {0:.4f}\".format(test_f1_macro))\n",
        "\n",
        "  tp += TP(logits, label_ids)\n",
        "  fp += FP(logits, label_ids)\n",
        "  fn += FN(logits, label_ids)\n",
        "  tn += TN(logits, label_ids) \n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print(\" test Accuracy: {0:.4f}\".format(avg_test_accuracy))\n",
        "print(\" test f1: {0:.4f}\".format(avg_test_f1))\n",
        "print(\" test f1_macro: {0:.4f}\".format(avg_test_f1_macro))\n",
        "#print(\" test tn: {0:.4f}\".format(avg_tn))\n",
        "#print(\" test fp: {0:.4f}\".format(avg_fp))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h5Z_BOslFEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tp,fp,fn,tn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jXM-YcW3ZAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tp+fp+fn+tn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cQjJWmfgiz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_confusion = (np.array([[tn,fp],[fn,tp]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-vHl7o_apgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred1 = np.array([[163,46],[77,574]])\n",
        "pred1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qhrj965criV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.subplot()\n",
        "sb = seaborn.heatmap(pred_confusion, annot=True, fmt=\".0f\")\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix') \n",
        "ax.xaxis.set_ticklabels(['NOT', 'OFF'])\n",
        "ax.yaxis.set_ticklabels(['NOT', 'OFF'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK9bTHCCBL0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seaborn.heatmap(pred1, annot=True, fmt=\".0f\")\n",
        "#plot_confusion_matrix(pred1,classes = [\"OFFENSIVE\",\"NOT\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}